{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, dataModel, numHeads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert dataModel % numHeads == 0, \"dataModel must be divisible by numHeads\"\n",
    "\n",
    "    self.dataModel = dataModel\n",
    "    self.numHeads = numHeads\n",
    "    self.dimK = dataModel // numHeads # int division\n",
    "    # Weights for Q, K, V\n",
    "    self.weightQ = nn.Linear(dataModel, dataModel)\n",
    "    self.weightK = nn.Linear(dataModel, dataModel)\n",
    "    self.weightV = nn.Linear(dataModel, dataModel)\n",
    "    self.weightO = nn.Linear(dataModel, dataModel)\n",
    "\n",
    "  def scaledDotProductAttention(self, Q, K, V, mask=None):\n",
    "    attentionScores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dimK)\n",
    "    if mask is not None:\n",
    "      attentionScores = attentionScores.masked_fill(mask, -1e9)\n",
    "    attentionProbabilities = nn.functional.softmax(attentionScores, dim=-1)\n",
    "    output = torch.matmul(attentionProbabilities, V)\n",
    "    return output\n",
    "  \n",
    "  def splitHeads(self, x, batchSize):\n",
    "    batchSize, seqLen, _ = x.size()\n",
    "    return x.view(batchSize, seqLen, self.numHeads, self.dimK).transpose(1, 2)\n",
    "  \n",
    "  def combineHeads(self, x):\n",
    "    batchSize, _, seqLen, _ = x.size()\n",
    "    return x.transpose(1, 2).contiguous().view(batchSize, seqLen, self.dataModel)\n",
    "  \n",
    "  def forward(self, Q, K, V, mask=None):\n",
    "    Q = self.splitHeads(self.weightQ(Q))\n",
    "    K = self.splitHeads(self.weightK(K))\n",
    "    V = self.splitHeads(self.weightV(V))\n",
    "    attentionOutput = self.scaledDotProductAttention(Q, K, V, mask)\n",
    "    output = self.weightO(self.combineHeads(attentionOutput))\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNN(nn.Module):\n",
    "  def __init__(self, dataModel, dFF):\n",
    "    super(PositionWiseFeedForwardNN, self).__init__()\n",
    "    self.forwardConnected1 = nn.Linear(dataModel, dFF)\n",
    "    self.forwardConnected2 = nn.Linear(dFF, dataModel)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.forwardConnected2(self.relu(self.forwardConnected1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, dataModel, maxSeqLen):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(maxSeqLen, dataModel)\n",
    "    position = torch.arange(start=0, end=maxSeqLen, step=1, dtype=torch.float32).unsqueeze(1) # Unsqueeze makes it into a column matrix\n",
    "    embeddingIndex = torch.arange(start=0, end=dataModel, step=2, dtype=torch.float32)\n",
    "    divTerm = 1 / torch.tensor(1e4)**(embeddingIndex/dataModel)\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * divTerm)\n",
    "    pe[:, 1::2] = torch.cos(position * divTerm)\n",
    "    self.register_buffer('pe', pe)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1e4)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, dataModel, numHeads, dFF, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.selfAttention = MultiHeadAttention(dataModel=dataModel, numHeads=numHeads)\n",
    "    self.feedForward = PositionWiseFeedForwardNN(dataModel=dataModel, dFF=dFF)\n",
    "    self.layerNorm1 = nn.LayerNorm(dataModel)\n",
    "    self.layerNorm2 = nn.LayerNorm(dataModel)\n",
    "    self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    attentionOut = self.selfAttention(x, x, x, mask)\n",
    "    x = self.layerNorm1(x + self.Dropout(attentionOut))\n",
    "    feedForwardOut = self.feedForward(x)\n",
    "    x = self.layerNorm2(x + self.Dropout(feedForwardOut))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, dataModel, numHeads, dFF, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.selfAttention = MultiHeadAttention(dataModel=dataModel, numHeads=numHeads)\n",
    "    self.crossAttention = MultiHeadAttention(dataModel=dataModel, numHeads=numHeads)\n",
    "    self.feedForward = PositionWiseFeedForwardNN(dataModel=dataModel, dFF=dFF)\n",
    "    self.layerNorm1 = nn.LayerNorm(dataModel)\n",
    "    self.layerNorm2 = nn.LayerNorm(dataModel)\n",
    "    self.layerNorm3 = nn.LayerNorm(dataModel)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, encoderOutput, srcMask, targetMask):\n",
    "    attentionOut = self.selfAttention(x, x, x, targetMask)\n",
    "    x = self.layerNorm1(x + self.dropout(attentionOut))\n",
    "    crossAttentionOut = self.crossAttention(x, encoderOutput, encoderOutput, srcMask)\n",
    "    x = self.layerNorm2(x + self.dropout(crossAttentionOut))\n",
    "    feedForwardOut = self.feedForward(x)\n",
    "    x = self.layerNorm3(x + self.dropout(feedForwardOut))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, sourceVocabSize, targetVocabSize, dataModel, numHeads, numLayers, dFF, maxSeqLen, dropout):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.encoderEmbedding = nn.Embedding(sourceVocabSize, dataModel)\n",
    "    self.decoderEmbedding = nn.Embedding(targetVocabSize, dataModel)\n",
    "    self.positionalEncoding = PositionalEncoding(dataModel=dataModel, maxSeqLen=maxSeqLen)\n",
    "    self.encoderLayers = nn.ModuleList([EncoderLayer(dataModel=dataModel, numHeads=numHeads, dFF=dFF, dropout=dropout) for _ in range(numLayers)])\n",
    "    self.decoderLayers = nn.ModuleList([DecoderLayer(dataModel=dataModel, numHeads=numHeads, dFF=dFF, dropout=dropout) for _ in range(numLayers)])\n",
    "    self.fc = nn.Linear(dataModel, targetVocabSize)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def generateMask(self, src, target):\n",
    "    srcMask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    targetMask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "    sequenceLength = target.size(1)\n",
    "    nopeakMask = (1 - torch.triu(torch.ones(sequenceLength, sequenceLength), diagonal=1)).bool()\n",
    "    targetMask = targetMask & nopeakMask\n",
    "    return srcMask, targetMask\n",
    "\n",
    "  def forward(self, src, target):\n",
    "    srcMask, targetMask = self.generateMask(src, target)\n",
    "    srcEmbedded = self.dropout(self.positionalEncoding(self.encoderEmbedding(src)))\n",
    "    targetEmbedded = self.dropout(self.positionalEncoding(self.decoderEmbedding(target)))\n",
    "\n",
    "    encoderOut = srcEmbedded\n",
    "    for encodedLayer in self.encoderLayers:\n",
    "      encoderOut = encodedLayer(encoderOut, srcMask)\n",
    "    decoderOut = targetEmbedded\n",
    "    for decodedLayer in self.decoderLayers:\n",
    "      decoderOut = decodedLayer(decoderOut, encoderOut, srcMask, targetMask)\n",
    "    output = self.fc(decoderOut)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceVocabSize = 5000\n",
    "targetVocabSize = 5000\n",
    "dataModel = 512\n",
    "numHeads = 8\n",
    "numLayers = 6\n",
    "dFF = 2**11\n",
    "maxSequenceLength = 100\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "  sourceVocabSize=sourceVocabSize,\n",
    "  targetVocabSize=targetVocabSize,\n",
    "  dataModel=dataModel,\n",
    "  numHeads=numHeads,\n",
    "  numLayers=numLayers,\n",
    "  dFF=dFF,\n",
    "  maxSeqLen=maxSequenceLength,\n",
    "  dropout=dropout\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceData = torch.randint(1, sourceVocabSize, (64, maxSequenceLength))\n",
    "targetData = torch.randint(1, targetVocabSize, (64, maxSequenceLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoderEmbedding): Embedding(5000, 512)\n",
       "  (decoderEmbedding): Embedding(5000, 512)\n",
       "  (positionalEncoding): PositionalEncoding()\n",
       "  (encoderLayers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (selfAttention): MultiHeadAttention(\n",
       "        (weightQ): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightK): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightV): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightO): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feedForward): PositionWiseFeedForwardNN(\n",
       "        (forwardConnected1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (forwardConnected2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoderLayers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (selfAttention): MultiHeadAttention(\n",
       "        (weightQ): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightK): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightV): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightO): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (crossAttention): MultiHeadAttention(\n",
       "        (weightQ): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightK): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightV): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (weightO): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feedForward): PositionWiseFeedForwardNN(\n",
       "        (forwardConnected1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (forwardConnected2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9,0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (100) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      2\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 3\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msourceData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetData\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m   loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, targetVocabSize), targetData[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Volumes/code/env/pytorch-metal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/code/env/pytorch-metal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, target)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, target):\n\u001b[1;32m     21\u001b[0m   srcMask, targetMask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerateMask(src, target)\n\u001b[0;32m---> 22\u001b[0m   srcEmbedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoderEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m   targetEmbedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositionalEncoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderEmbedding(target)))\n\u001b[1;32m     25\u001b[0m   encoderOut \u001b[38;5;241m=\u001b[39m srcEmbedded\n",
      "File \u001b[0;32m/Volumes/code/env/pytorch-metal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/code/env/pytorch-metal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (100) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "  optimizer.zero_grad()\n",
    "  output = transformer(sourceData, targetData[:, :-1])\n",
    "  loss = criterion(output.contiguous().view(-1, targetVocabSize), targetData[:, 1:].contiguous().view(-1))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
